% Logistic Regression With Covariates
% Andy Grogan-Kaylor
% `s c(current_date)` `s c(current_time)`

---
geometry: margin=1 in
---

<style>h1 {color: #00274C;} h2 {color: #2F65A7;} blockquote {border-left: 5px solid #ffcb05; margin: 1.5em 10px; padding: 0.5em 10px;}</style>

# Background

In linear regression, interpretation of coefficients is *somewhat* straightforward. We might first estimate:

$y = \beta_0 + \beta_1 x_1 + e_i$

and then: 

$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + e_i$

and would say--in the second equation--that $\beta_1$ is an estimate that accounts for the association of $x_2$ and $y$.

However, in logistic regression, the situation is somewhat different.

As Allison (1999) notes:

> Unfortunately, there is a potential pitfall in cross-group comparisons of logit or probit coefficients that has largely gone unnoticed. Unlike linear regression coefficients, coefficients in these binary regression models are confounded with residual variation (unobserved heterogeneity). Differences in the degree of residual variation across groups can produce apparent differences in coefficients that are not indicative of true differences in causal effects. 

While the mathematics of this relationship are somewhat difficult--though clearly presented in Allison's (1999) article--the finding can be easily seen in simulated data.

# Simulate Data

    clear all

    cd "/Users/agrogan/Desktop/newstuff/categorical/logistic-and-covariates"

	set obs 10000
	
	set seed 3846 // random seed
	
	generate x1 = rnormal() // normally distributed x

	histogram x1, scheme(michigan)
	
	graph export histogram1.png, width(500) replace
	
![Histogram of x1](histogram1.png){width=25%}

	generate x2 = rnormal() // normally distributed z
	
	histogram x2, scheme(michigan)

	graph export histogram2.png, width(500) replace
	
![Histogram of x2](histogram2.png){width=25%}
	
	generate e = rnormal(0, .5) // normally distributed error
	
> Since they were generated independently, $x_1$ and $x_2$ are relatively uncorrelated.
	
    corr x1 x2 // x1 and x2 are *mostly* uncorrelated
		
# Linear Regression

	generate y1 = x1 + x2 + e // dependent variable
	
	regress y1 x1
	
> A 1 unit change in $x_1$ is associated with a 1.02 change in $y_1$.
	
	est store OLS1 // store estimates
	
	regress y1 x1 x2 
	
> A 1 unit change in $x_1$ is associated with a 1.01 change in $y_1$. The slight change in coefficient for $x_1$ is likely due to the very slight correlation between $x_1$ and $x_2$.
	
	est store OLS2 // store estimates
	
> Note that the coefficients for $x_1$ in the two models are relatively close.
	
	estimates table OLS1 OLS2, b(%7.4f) star // table comparing estimates
	
# Logistic Regression
	
    generate prob_y2 = exp(x1 + x2 + e) / (1 + exp(x1 + x2 + e)) // dependent variable 
	
	recode prob_y2 (0/.5 =0)(.5/1 = 1), generate(y2) // recode probabilites as observed values
	
	logit y2 x1
	
> A 1 unit change in $x_1$ is associated with a 1.53 change in the *log odds* of $y_2$.
	
	est store logit1
	
    logit y2 x1 x2
	
> A 1 unit change in $x_1$ is associated with a 3.69 change in the *log odds* of $y_2$.

	est store logit2
	
> Note that the coefficients for $x_1$ in the two models are rather different, even though $x_1$ and $x_2$ have, by definition, a very very small correlation.
	
	estimates table logit1 logit2, b(%7.4f) star // table comparing estimates

# References

Allison, P. D. (1999). Comparing logit and probit coefficients across groups. *Sociological Methods and Research*. [https://doi.org/10.1177/0049124199028002003](https://doi.org/10.1177/0049124199028002003)
