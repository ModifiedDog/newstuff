{smcl}
{com}{sf}{ul off}{txt}{.-}
      name:  {res}<unnamed>
       {txt}log:  {res}/Users/agrogan/Desktop/newstuff/categorical/Bayes/Bayes.smcl
  {txt}log type:  {res}smcl
 {txt}opened on:  {res}16 Jul 2020, 15:42:27
{txt}
{com}. //_1
. display c(current_date)
{res}16 Jul 2020
{txt}
{com}. //_2
. display c(current_time)
{res}15:42:27
{txt}
{com}. //_3
. clear all
{res}{txt}
{com}. //_4
. clear all
{res}{txt}
{com}. //_5
. use "../logistic-regression/GSSsmall.dta", clear
{txt}
{com}. //_6
. logit liberal i.race i.class

{res}{txt}Iteration 0:{space 3}log likelihood = {res:-31538.733}  
Iteration 1:{space 3}log likelihood = {res:-31370.507}  
Iteration 2:{space 3}log likelihood = {res:-31369.841}  
Iteration 3:{space 3}log likelihood = {res:-31369.841}  
{res}
{txt}Logistic regression{col 49}Number of obs{col 67}= {res}    53,625
{txt}{col 49}LR chi2({res}5{txt}){col 67}= {res}    337.78
{txt}{col 49}Prob > chi2{col 67}= {res}    0.0000
{txt}Log likelihood = {res}-31369.841{txt}{col 49}Pseudo R2{col 67}= {res}    0.0054

{txt}{hline 15}{c TT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{col 1}       liberal{col 16}{c |}      Coef.{col 28}   Std. Err.{col 40}      z{col 48}   P>|z|{col 56}     [95% Con{col 69}f. Interval]
{hline 15}{c +}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}
{space 10}race {c |}
{space 8}black  {c |}{col 16}{res}{space 2} .4443531{col 28}{space 2} .0272062{col 39}{space 1}   16.33{col 48}{space 3}0.000{col 56}{space 4}   .39103{col 69}{space 3} .4976762
{txt}{space 8}other  {c |}{col 16}{res}{space 2} .3190896{col 28}{space 2} .0413275{col 39}{space 1}    7.72{col 48}{space 3}0.000{col 56}{space 4} .2380891{col 69}{space 3} .4000901
{txt}{space 14} {c |}
{space 9}class {c |}
working class  {c |}{col 16}{res}{space 2}-.1397848{col 28}{space 2}  .041515{col 39}{space 1}   -3.37{col 48}{space 3}0.001{col 56}{space 4}-.2211527{col 69}{space 3}-.0584169
{txt}{space 1}middle class  {c |}{col 16}{res}{space 2}-.0117948{col 28}{space 2} .0416509{col 39}{space 1}   -0.28{col 48}{space 3}0.777{col 56}{space 4} -.093429{col 69}{space 3} .0698394
{txt}{space 2}upper class  {c |}{col 16}{res}{space 2} .1512565{col 28}{space 2} .0648962{col 39}{space 1}    2.33{col 48}{space 3}0.020{col 56}{space 4} .0240624{col 69}{space 3} .2784507
{txt}{space 14} {c |}
{space 9}_cons {c |}{col 16}{res}{space 2}-.9900441{col 28}{space 2} .0397384{col 39}{space 1}  -24.91{col 48}{space 3}0.000{col 56}{space 4} -1.06793{col 69}{space 3}-.9121582
{txt}{hline 15}{c BT}{hline 11}{hline 11}{hline 9}{hline 8}{hline 13}{hline 12}

{com}. //_7
. sample 10 // Random Sample To Speed This Example: DON'T DO THIS IN PRACTICE!!!
{txt}(58,332 observations deleted)

{com}. //_8
. bayes: logit liberal i.race i.class
{res}  
{txt}Burn-in ...
{txt}Simulation ...
{res}
{txt}Model summary
{txt}{hline 80}
{txt}Likelihood: 
{p 0 12}{space 2}{res:liberal} ~ logit(xb_liberal){p_end}

Prior: 
{p 0 35}{space 2}{res}{c -(}liberal:i.race i.class _cons{c )-}{txt} ~ normal(0,10000){space 27}(1){p_end}
{txt}{hline 80}
{p 0 4 0 80}
(1) Parameters are elements of the linear form xb_liberal.
{p_end}

{res}{txt}Bayesian logistic regression{col 52}MCMC iterations{col 69}={col 71}{res}    12,500
{txt}Random-walk Metropolis-Hastings sampling{col 52}Burn-in{col 69}={col 71}{res}     2,500
{col 52}{txt}MCMC sample size{col 69}={col 71}{res}    10,000
{txt}{col 52}Number of obs{col 69}={col 71}{res}     5,393
{txt}{col 52}Acceptance rate{col 69}={col 71}{res}     .2257
{txt}{col 52}Efficiency:{col 65}min ={col 71}{res}    .01383
{col 65}{txt}avg ={col 71}{res}    .03232
{txt}Log marginal likelihood = {res}-3180.1718{col 65}{txt}max ={col 71}{res}     .0583
 
{txt}{hline 15}{col 16}{c TT}{hline 64}
{col 16}{c |}{col 65}Equal-tailed
{col 8}liberal{col 16}{c |}{col 23}Mean{col 30}Std. Dev.{col 44}MCSE{col 53}Median{col 61}[95% Cred. Interval]
{res}{txt}{hline 15}{c +}{hline 10}{hline 11}{hline 10}{hline 11}{hline 11}{hline 11}
{space 10}race {c |}
{space 8}black  {c |}{col 16}{res}{space 1} .3947493{col 27}{space 2} .0822819{col 38}{space 2} .005895{col 48}{space 2} .3933429{col 59}{space 2} .2289023{col 70}{space 2} .5567789
{txt}{space 8}other  {c |}{col 16}{res}{space 1} .5018242{col 27}{space 2} .1295803{col 38}{space 2} .011017{col 48}{space 2}  .503707{col 59}{space 2}  .255992{col 70}{space 2} .7522975
{txt}{space 14} {c |}
{space 9}class {c |}
working class  {c |}{col 16}{res}{space 1}-.4241844{col 27}{space 2} .1243631{col 38}{space 2} .007727{col 48}{space 2} -.422026{col 59}{space 2}-.6713128{col 70}{space 2}  -.20072
{txt}{space 1}middle class  {c |}{col 16}{res}{space 1}-.2619162{col 27}{space 2} .1232291{col 38}{space 2} .006378{col 48}{space 2}-.2664475{col 59}{space 2}-.5197306{col 70}{space 2}-.0254913
{txt}{space 2}upper class  {c |}{col 16}{res}{space 1}-.1508993{col 27}{space 2} .1983133{col 38}{space 2} .010032{col 48}{space 2}-.1534356{col 59}{space 2}-.5326012{col 70}{space 2} .2457839
{txt}{space 14} {c |}
{space 9}_cons {c |}{col 16}{res}{space 1}-.7522187{col 27}{space 2} .1149148{col 38}{space 2} .004759{col 48}{space 2}-.7508257{col 59}{space 2}-.9723163{col 70}{space 2}-.5232482
{txt}{hline 15}{c BT}{hline 10}{hline 11}{hline 10}{hline 11}{hline 11}{hline 11}
{p 0 6 2}Note: {help j_bayes_defaultpriors:Default priors} are used for model parameters.{p_end}
{res}{txt}
{com}. //_9
. * bayes, block({c -(}liberal:i.race{c )-}): logit liberal i.race i.class // blocking may improve estimation
. //_10
. bayes, normalprior(5): logit liberal i.race i.class
{res}  
{txt}Burn-in ...
{txt}Simulation ...
{res}
{txt}Model summary
{txt}{hline 80}
{txt}Likelihood: 
{p 0 12}{space 2}{res:liberal} ~ logit(xb_liberal){p_end}

Prior: 
{p 0 35}{space 2}{res}{c -(}liberal:i.race i.class _cons{c )-}{txt} ~ normal(0,25){space 30}(1){p_end}
{txt}{hline 80}
{p 0 4 0 80}
(1) Parameters are elements of the linear form xb_liberal.
{p_end}

{res}{txt}Bayesian logistic regression{col 52}MCMC iterations{col 69}={col 71}{res}    12,500
{txt}Random-walk Metropolis-Hastings sampling{col 52}Burn-in{col 69}={col 71}{res}     2,500
{col 52}{txt}MCMC sample size{col 69}={col 71}{res}    10,000
{txt}{col 52}Number of obs{col 69}={col 71}{res}     5,393
{txt}{col 52}Acceptance rate{col 69}={col 71}{res}     .2018
{txt}{col 52}Efficiency:{col 65}min ={col 71}{res}    .01531
{col 65}{txt}avg ={col 71}{res}     .0254
{txt}Log marginal likelihood = {res}-3162.2981{col 65}{txt}max ={col 71}{res}    .04229
 
{txt}{hline 15}{col 16}{c TT}{hline 64}
{col 16}{c |}{col 65}Equal-tailed
{col 8}liberal{col 16}{c |}{col 23}Mean{col 30}Std. Dev.{col 44}MCSE{col 53}Median{col 61}[95% Cred. Interval]
{res}{txt}{hline 15}{c +}{hline 10}{hline 11}{hline 10}{hline 11}{hline 11}{hline 11}
{space 10}race {c |}
{space 8}black  {c |}{col 16}{res}{space 1} .4063884{col 27}{space 2} .0830629{col 38}{space 2} .004039{col 48}{space 2} .4098444{col 59}{space 2} .2476307{col 70}{space 2} .5649653
{txt}{space 8}other  {c |}{col 16}{res}{space 1} .5087264{col 27}{space 2} .1206866{col 38}{space 2} .007204{col 48}{space 2} .5028487{col 59}{space 2} .2772664{col 70}{space 2} .7407117
{txt}{space 14} {c |}
{space 9}class {c |}
working class  {c |}{col 16}{res}{space 1}-.4232222{col 27}{space 2} .1221491{col 38}{space 2} .008601{col 48}{space 2} -.424899{col 59}{space 2}-.6637786{col 70}{space 2}-.1720494
{txt}{space 1}middle class  {c |}{col 16}{res}{space 1}-.2570835{col 27}{space 2} .1197257{col 38}{space 2} .009677{col 48}{space 2}-.2614378{col 59}{space 2}-.4845897{col 70}{space 2}-.0119759
{txt}{space 2}upper class  {c |}{col 16}{res}{space 1} -.135455{col 27}{space 2}  .200802{col 38}{space 2} .012247{col 48}{space 2}-.1340006{col 59}{space 2}-.5505343{col 70}{space 2} .2664472
{txt}{space 14} {c |}
{space 9}_cons {c |}{col 16}{res}{space 1}-.7615401{col 27}{space 2} .1153949{col 38}{space 2} .008224{col 48}{space 2}-.7556258{col 59}{space 2} -.998298{col 70}{space 2}-.5372218
{txt}{hline 15}{c BT}{hline 10}{hline 11}{hline 10}{hline 11}{hline 11}{hline 11}
{p 0 6 2}Note: {help j_bayes_defaultpriors:Default priors} are used for model parameters.{p_end}
{res}{txt}
{com}. //_11
. clear all
{res}{txt}
{com}. //_12
. set obs 100
{txt}{p}
number of observations (_N)  was 0,
now 100
{p_end}

{com}. //_13
. generate myestimate = rnormal() + 10 // simulated values of estimate
{txt}
{com}. //_14
. summarize myestimate

{txt}    Variable {c |}        Obs        Mean    Std. Dev.       Min        Max
{hline 13}{c +}{hline 57}
{space 2}myestimate {c |}{res}        100     9.99037    .9061558   7.483831   11.92453
{txt}
{com}. //_15
. local mymean = r(mean)
{txt}
{com}. //_16
. kdensity myestimate ,  ///
> title("Likelihood of Estimate") ///
> xtitle("Estimate") ytitle("Likelihood") ///
> note("Vertical Line At Mean Value") ///
> caption("ML gives point estimate; Bayes gives full range of distribution") ///
> xline(`mymean', lwidth(1) lcolor(gold)) scheme(michigan)
{res}{txt}
{com}. //_17
. graph export MCMC-ML.png, width(500) replace
{txt}(file MCMC-ML.png written in PNG format)

{com}. //_18
. clear all
{res}{txt}
{com}. //_19
. use "../logistic-regression/GSSsmall.dta", clear
{txt}
{com}. //_20
. sample 10 // Random Sample for These Slides: DON'T DO THIS IN PRACTICE!!!
{txt}(58,332 observations deleted)

{com}. //_21
. bayes, normalprior(5): logit liberal i.race i.class
{res}  
{txt}Burn-in ...
{txt}Simulation ...
{res}
{txt}Model summary
{txt}{hline 80}
{txt}Likelihood: 
{p 0 12}{space 2}{res:liberal} ~ logit(xb_liberal){p_end}

Prior: 
{p 0 35}{space 2}{res}{c -(}liberal:i.race i.class _cons{c )-}{txt} ~ normal(0,25){space 30}(1){p_end}
{txt}{hline 80}
{p 0 4 0 80}
(1) Parameters are elements of the linear form xb_liberal.
{p_end}

{res}{txt}Bayesian logistic regression{col 52}MCMC iterations{col 69}={col 71}{res}    12,500
{txt}Random-walk Metropolis-Hastings sampling{col 52}Burn-in{col 69}={col 71}{res}     2,500
{col 52}{txt}MCMC sample size{col 69}={col 71}{res}    10,000
{txt}{col 52}Number of obs{col 69}={col 71}{res}     5,345
{txt}{col 52}Acceptance rate{col 69}={col 71}{res}     .2082
{txt}{col 52}Efficiency:{col 65}min ={col 71}{res}    .02443
{col 65}{txt}avg ={col 71}{res}    .03407
{txt}Log marginal likelihood = {res}-3105.9749{col 65}{txt}max ={col 71}{res}    .06349
 
{txt}{hline 15}{col 16}{c TT}{hline 64}
{col 16}{c |}{col 65}Equal-tailed
{col 8}liberal{col 16}{c |}{col 23}Mean{col 30}Std. Dev.{col 44}MCSE{col 53}Median{col 61}[95% Cred. Interval]
{res}{txt}{hline 15}{c +}{hline 10}{hline 11}{hline 10}{hline 11}{hline 11}{hline 11}
{space 10}race {c |}
{space 8}black  {c |}{col 16}{res}{space 1} .5422929{col 27}{space 2} .0827044{col 38}{space 2} .004872{col 48}{space 2} .5434182{col 59}{space 2} .3823331{col 70}{space 2} .7078339
{txt}{space 8}other  {c |}{col 16}{res}{space 1} .4157509{col 27}{space 2} .1281866{col 38}{space 2} .005087{col 48}{space 2} .4171814{col 59}{space 2}  .163697{col 70}{space 2} .6570723
{txt}{space 14} {c |}
{space 9}class {c |}
working class  {c |}{col 16}{res}{space 1}-.2171419{col 27}{space 2} .1361014{col 38}{space 2} .008708{col 48}{space 2}-.2179448{col 59}{space 2}-.4807177{col 70}{space 2} .0627315
{txt}{space 1}middle class  {c |}{col 16}{res}{space 1}-.1583411{col 27}{space 2} .1396594{col 38}{space 2} .008239{col 48}{space 2}-.1582649{col 59}{space 2}-.4454747{col 70}{space 2} .1111331
{txt}{space 2}upper class  {c |}{col 16}{res}{space 1}-.0034842{col 27}{space 2} .2229609{col 38}{space 2} .012502{col 48}{space 2}-.0032252{col 59}{space 2}-.4430678{col 70}{space 2} .4161878
{txt}{space 14} {c |}
{space 9}_cons {c |}{col 16}{res}{space 1}-.9453812{col 27}{space 2} .1325399{col 38}{space 2} .008048{col 48}{space 2}-.9430257{col 59}{space 2}-1.212341{col 70}{space 2}-.6728245
{txt}{hline 15}{c BT}{hline 10}{hline 11}{hline 10}{hline 11}{hline 11}{hline 11}
{p 0 6 2}Note: {help j_bayes_defaultpriors:Default priors} are used for model parameters.{p_end}
{res}{txt}
{com}. //_22
. bayesgraph kdensity {c -(}liberal:2.race{c )-}, scheme(michigan)
{res}{txt}
{com}. //_23
. graph export mybayesgraph.png, width(500) replace
{txt}(file mybayesgraph.png written in PNG format)

{com}. //_^
. log close
      {txt}name:  {res}<unnamed>
       {txt}log:  {res}/Users/agrogan/Desktop/newstuff/categorical/Bayes/Bayes.smcl
  {txt}log type:  {res}smcl
 {txt}closed on:  {res}16 Jul 2020, 15:43:25
{txt}{.-}
{smcl}
{txt}{sf}{ul off}